<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Math on The Journeyman Modeler</title><link>http://jmodeler.github.io/categories/math/</link><description>Recent content in Math on The Journeyman Modeler</description><generator>Hugo</generator><language>en</language><lastBuildDate>Mon, 12 May 2025 00:00:00 +0000</lastBuildDate><atom:link href="http://jmodeler.github.io/categories/math/index.xml" rel="self" type="application/rss+xml"/><item><title>Inverse of a Transposed Matrix</title><link>http://jmodeler.github.io/posts/2025-05-12-inverse-of-a-transposed-matrix/</link><pubDate>Mon, 12 May 2025 00:00:00 +0000</pubDate><guid>http://jmodeler.github.io/posts/2025-05-12-inverse-of-a-transposed-matrix/</guid><description>&lt;p&gt;In this post I attempt to prove a very simple identity, that will be used in a later post.&lt;/p&gt;
&lt;p&gt;I want to prove that for a square, non-singular matrix &lt;code&gt;\(A\)&lt;/code&gt;, we have (in the steps below, &lt;code&gt;\(I\)&lt;/code&gt; is the identity matrix):&lt;/p&gt;
&lt;p&gt;&lt;code&gt;\begin{align} (A^T)^{-1} = (A^{-1})^{T} \end{align}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;code&gt;\((A^T)^{-1} = B\)&lt;/code&gt;, we have
&lt;code&gt;\begin{align} (A^T)^{-1} &amp;amp; = B \\ \implies (A^T)^{-1} A^T &amp;amp; = B A^T \\ \implies I &amp;amp; = B A^T \tag{1} \\ \implies I^T &amp;amp; = (BA^T)^T \\ \implies I &amp;amp; = AB^T \tag{2} \\ \implies A^{-1}I &amp;amp; = A^{-1}AB^T \\ \implies A^{-1} &amp;amp; = B^T \\ \implies (A^{-1})^T &amp;amp; = (B^T)^T \\ \implies (A^{-1})^T &amp;amp; = B \\ \implies (A^{-1})^T &amp;amp; = B = (A^T)^{-1} \end{align}&lt;/code&gt;&lt;/p&gt;</description></item><item><title>Distribution Of The Negative of A Normally Distributed Random Variable</title><link>http://jmodeler.github.io/posts/negative-normal-rv/</link><pubDate>Mon, 14 Apr 2025 00:00:00 +0000</pubDate><guid>http://jmodeler.github.io/posts/negative-normal-rv/</guid><description>&lt;p&gt;This post derives the distribution for an random variable that appears in the derivation of a &lt;a href="https://jmodeler.github.io/posts/2025-04-26-sample-size-and-effect-size/" class="external-link" target="_blank" rel="noopener"&gt;result in another post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this post, I derive the distribution of the negative of a normally distributed random variable. Let&amp;rsquo;s say we have &lt;code&gt;\(X \sim N(\mu, \sigma^2)\)&lt;/code&gt;, and we want to find the distribution of &lt;code&gt;\(Y=-X\)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;I start with the identity:
&lt;code&gt;\begin{align} P(Y \le y) = P(-X \le y) = P(X \ge -y) = 1 - P(X \le -y) \end{align}&lt;/code&gt;&lt;/p&gt;</description></item><item><title>Distribution of the Sample Mean of IID Normal Random Variables</title><link>http://jmodeler.github.io/posts/distribution-sample-mean/</link><pubDate>Mon, 14 Apr 2025 00:00:00 +0000</pubDate><guid>http://jmodeler.github.io/posts/distribution-sample-mean/</guid><description>&lt;p&gt;In this post I derive a result that will be used in &lt;a href="https://jmodeler.github.io/posts/2025-04-26-sample-size-and-effect-size/" class="external-link" target="_blank" rel="noopener"&gt;another post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Say we have Independent and Identically Distributed (IID) Normal random variables &lt;code&gt;\(X_i \sim N(\mu, \sigma^2)\)&lt;/code&gt;, and we have&lt;/p&gt;
&lt;p&gt;&lt;code&gt;\begin{align} \bar{X} = \frac{1}{n}\sum^{n}_{i=1} X_i \end{align}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;We want the distribution of &lt;code&gt;\(\bar{X}\)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;From a &lt;a href="https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables" class="external-link" target="_blank" rel="noopener"&gt;standard result&lt;/a&gt;, &lt;code&gt;\(\bar{X}\)&lt;/code&gt; is also Normally distributed. Once we have the mean and variance of &lt;code&gt;\(\bar{X}\)&lt;/code&gt;, we can completely specify its distribution.&lt;/p&gt;
&lt;p&gt;For the mean of &lt;code&gt;\(\bar{X}\)&lt;/code&gt;, note that &lt;code&gt;\(E(X_i) = \mu\)&lt;/code&gt; we then have:
&lt;code&gt;\begin{align} E(\bar{X}) = &amp;amp; \frac{1}{n} E\left(\sum^{n}_{i=1} X_i \right) \\ = &amp;amp; \frac{1}{n} \left(\sum^{n}_{i=1} E(X_i) \right) \\ = &amp;amp; \frac{1}{n} n\mu = \mu \end{align}&lt;/code&gt;&lt;/p&gt;</description></item><item><title>A Summation Formula</title><link>http://jmodeler.github.io/posts/a-summation-formula/</link><pubDate>Mon, 07 Apr 2025 00:00:00 +0000</pubDate><guid>http://jmodeler.github.io/posts/a-summation-formula/</guid><description>&lt;p&gt;This post derives a closed form solution for particular sum that appears in the derivation of a result in &lt;a href="https://jmodeler.github.io/posts/distribution-sample-mean/" class="external-link" target="_blank" rel="noopener"&gt;another post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Say we have real numbers &lt;code&gt;\(X_i\)&lt;/code&gt; such that
&lt;code&gt;\begin{align} X_i = x \,\, \forall i \tag{1} \end{align}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;And we want to find the sum
&lt;code&gt;\begin{align} \mathop{\sum\sum}_{\substack{i \ne j}} X_i X_j \,\,\,\, i,j \in {1, 2, \dots , n} \tag{2} \end{align}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;From (1), the sum in (2) boils down to:
&lt;code&gt;\begin{align} \mathop{\sum\sum}_{\substack{i \ne j}} x^2 \tag{3} \end{align}&lt;/code&gt;&lt;/p&gt;</description></item></channel></rss>