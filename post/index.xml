<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts | The Journeyman Modeler</title><link>https://jmodeler.github.io/post/</link><atom:link href="https://jmodeler.github.io/post/index.xml" rel="self" type="application/rss+xml"/><description>Posts</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 20 Mar 2023 00:00:00 +0000</lastBuildDate><image><url>https://jmodeler.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url><title>Posts</title><link>https://jmodeler.github.io/post/</link></image><item><title>Numerical Integration with Sparse Grids</title><link>https://jmodeler.github.io/post/numerical-integration-with-sparse-grids/</link><pubDate>Mon, 20 Mar 2023 00:00:00 +0000</pubDate><guid>https://jmodeler.github.io/post/numerical-integration-with-sparse-grids/</guid><description>
&lt;p>I recently read a paper &lt;span class="citation">(&lt;a href="#ref-heiss2008likelihood" role="doc-biblioref">Heiss and Winschel 2008&lt;/a>)&lt;/span> that advocated the use of certain techniques (Sparse Grids, SG henceforth) in numerical integration to calculate likelihood functions, as opposed to using Monte Carlo (MC henceforth) methods for the same. While approximating integrals with MC methods are simpler to implement, they might lead to integral values with considerable simulation error &lt;span class="citation">(&lt;a href="#ref-skrainka2011high" role="doc-biblioref">Skrainka and Judd 2011&lt;/a>)&lt;/span>. This post attempts to demonstrate the claim in &lt;span class="citation">Skrainka and Judd (&lt;a href="#ref-skrainka2011high" role="doc-biblioref">2011&lt;/a>)&lt;/span> using two very simple integrals, to which we already know the value. I attempt to compare the outcomes from using MC and SG.&lt;/p>
&lt;p>The integrals I’ll be evaluating are:&lt;/p>
&lt;p>&lt;span class="math display" id="eq:sumint">\[\begin{equation}
\int_{-\infty}^{\infty} \left( \sum_{i=1}^{5} x_i \right) dF_{X} \tag{1}
\end{equation}\]&lt;/span>&lt;/p>
&lt;p>and
&lt;span class="math display" id="eq:prodint">\[\begin{equation}
\int_{-\infty}^{\infty} \left( \prod_{i=1}^{5} x_i^2 \right) dF_{X} \tag{2}
\end{equation}\]&lt;/span>&lt;/p>
&lt;p>where &lt;span class="math inline">\(X = \{x_i\}_{i=1}^{5}\)&lt;/span> is a five dimensional random variable, which is distributed according to the multivariate standard normal:
&lt;span class="math display">\[
X \sim N\left( \left[ \begin{array}
{r}
0 \\
0 \\
0 \\
0 \\
0 \\
\end{array}\right], \left[ \begin{array}
{rrrrr}
1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 \\
\end{array}\right] \right)
\]&lt;/span>&lt;/p>
&lt;p>Given the distribution of &lt;span class="math inline">\(X\)&lt;/span>, the values of the integrals above are easily obtained from standard results (the value of &lt;a href="#eq:sumint">(1)&lt;/a> is &lt;span class="math inline">\(0\)&lt;/span> and that of &lt;a href="#eq:prodint">(2)&lt;/a> is &lt;span class="math inline">\(1\)&lt;/span>) respectively.&lt;/p>
&lt;p>I write some utility functions in R to compute the integrands above:&lt;/p>
&lt;pre class="r">&lt;code>#function to compute the sum of components of the random vector
s &amp;lt;- function(x)
{
return(sum(x))
}
#function to compute the square product of the components of the random vector
p &amp;lt;- function(x)
{
return(prod(x^2))
}&lt;/code>&lt;/pre>
&lt;p>I now write a function that:&lt;/p>
&lt;ul>
&lt;li>Simulates a certain number of draws from the distribution of the random variable &lt;span class="math inline">\(X\)&lt;/span>&lt;/li>
&lt;li>Computes the integrand function using each of these draws as input&lt;/li>
&lt;li>Takes the average of the values computed in the previous step&lt;/li>
&lt;/ul>
&lt;p>This function, in effect, would give us the approximate value of the integral via MC methodology.&lt;/p>
&lt;p>The code is provided below, note that I use the &lt;tt>&lt;a href="https://cran.r-project.org/web/packages/mvtnorm/index.html">mvtnorm&lt;/a>&lt;/tt> package to create random draws.&lt;/p>
&lt;pre class="r">&lt;code>library(mvtnorm)
#Function to calculate the MC approximation for the integral
mc_int &amp;lt;- function(s, n, mu, sigma)
{
#generate random draws
x &amp;lt;- rmvnorm(n, mean = mu, sigma = sigma)
#now get the integral
mc_int_n &amp;lt;- mean(apply(x, 1, s))
return(mc_int_n)
}
set.seed(100)
n &amp;lt;- 1000
mc_val &amp;lt;- mc_int(s, n, mu = rep(0,5), sigma = diag(5))
mc_val&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 0.007150433&lt;/code>&lt;/pre>
&lt;p>The result, 0.00715 is not far off from the true value of &lt;span class="math inline">\(0\)&lt;/span> at first glance, however, we need to compare this to the result from the SG approach.&lt;/p>
&lt;p>R has a package that generates sparse grids for numerical integration as described in &lt;span class="citation">Heiss and Winschel (&lt;a href="#ref-heiss2008likelihood" role="doc-biblioref">2008&lt;/a>)&lt;/span>, called &lt;tt>&lt;a href="https://cran.r-project.org/web/packages/SparseGrid/index.html">SparseGrid&lt;/a>&lt;/tt>. We now use the nodes and weights generated from this package to approximate the first integral.
I re-use some of the code provided in the documentation for the &lt;tt>&lt;a href="https://cran.r-project.org/web/packages/SparseGrid/vignettes/SparseGrid.pdf">SparseGrid&lt;/a>&lt;/tt> package in R.&lt;/p>
&lt;pre class="r">&lt;code>library(SparseGrid)
#generate sparse grids for a 5 dimensional RV with accuracy level 2
sg &amp;lt;- createSparseGrid(type=&amp;#39;KPN&amp;#39;, dimension=5, k=2)
sg_int &amp;lt;- function(func, sg, ...)
{
gx &amp;lt;- apply(sg$nodes, 1, function(x) {func(x, ...)})
return(sum(gx * sg$weights))
}
sg_val &amp;lt;- sg_int(s, sg)
sg_val&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 0&lt;/code>&lt;/pre>
&lt;p>The result here is exactly 0. In light of this finding, the value obtained from the MC approach, in comparison, is a little off, and tends to show a high variance in output:&lt;/p>
&lt;pre class="r">&lt;code>set.seed(100)
mc_int(s, n, mu = rep(0,5), sigma = diag(5))&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 0.007150433&lt;/code>&lt;/pre>
&lt;pre class="r">&lt;code>mc_int(s, n, mu = rep(0,5), sigma = diag(5))&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 0.03162326&lt;/code>&lt;/pre>
&lt;pre class="r">&lt;code>mc_int(s, n, mu = rep(0,5), sigma = diag(5))&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] -0.1287932&lt;/code>&lt;/pre>
&lt;pre class="r">&lt;code>mc_int(s, n, mu = rep(0,5), sigma = diag(5))&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 0.01040134&lt;/code>&lt;/pre>
&lt;pre class="r">&lt;code>mc_int(s, n, mu = rep(0,5), sigma = diag(5))&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] -0.03798655&lt;/code>&lt;/pre>
&lt;p>In the third case, there is a &lt;span class="math inline">\(-12\%\)&lt;/span> error(!) in the value of the computed integral when compared to the result from the SG approach. The SG approach, in addition, shows no such variation in repeated runs, since the grid values and weights are fixed for a given accuracy level and dimension (of the variable being integrated).&lt;/p>
&lt;p>I repeat the calculations for the second integral, as shown below&lt;/p>
&lt;p>MC approach:&lt;/p>
&lt;pre class="r">&lt;code>set.seed(100)
n &amp;lt;- 1000
mc_val &amp;lt;- mc_int(p, n, mu = rep(0,5), sigma = diag(5))
mc_val&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 1.001089&lt;/code>&lt;/pre>
&lt;p>SG approach:&lt;/p>
&lt;pre class="r">&lt;code>#generate sparse grids for a 5 dimensional RV with accuracy level 6
sg &amp;lt;- createSparseGrid(type=&amp;#39;KPN&amp;#39;, dimension=5, k=6)
sg_val &amp;lt;- sg_int(p, sg)
sg_val&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 1&lt;/code>&lt;/pre>
&lt;p>Once again, the SG approach gives us an exact value (note that the value of &lt;span class="math inline">\(k\)&lt;/span>, the accuracy level, has gone up, since the integrand is a higher order function). Again, the difference of the results between the two approaches doesn’t seem that large. However, variability of the results from the MC approach is still a concern, as shown below:&lt;/p>
&lt;pre class="r">&lt;code>set.seed(100)
mc_int(p, n, mu = rep(0,5), sigma = diag(5))&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 1.001089&lt;/code>&lt;/pre>
&lt;pre class="r">&lt;code>mc_int(p, n, mu = rep(0,5), sigma = diag(5))&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 0.4672555&lt;/code>&lt;/pre>
&lt;pre class="r">&lt;code>mc_int(p, n, mu = rep(0,5), sigma = diag(5))&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 1.14692&lt;/code>&lt;/pre>
&lt;pre class="r">&lt;code>mc_int(p, n, mu = rep(0,5), sigma = diag(5))&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 1.062975&lt;/code>&lt;/pre>
&lt;pre class="r">&lt;code>mc_int(p, n, mu = rep(0,5), sigma = diag(5))&lt;/code>&lt;/pre>
&lt;pre>&lt;code>## [1] 0.9112416&lt;/code>&lt;/pre>
&lt;p>In the second case, there is a roughly &lt;span class="math inline">\(53\%\)&lt;/span> (!!) error when compared to the true value of the integral. This variability could be worse with more complicated integrands. One suggestion to reduce variability in MC methods is to increase the number of draws, but that would entail a lot of calculations and result in longer runtimes.&lt;/p>
&lt;p>To conclude, we have shown with this simple example how results from numerical integration using MC methods have high variability, and more often than not, it would be good idea to increase the number of draws being used to approximate an integral, or use a different method altogether (SG).&lt;/p>
&lt;div id="references" class="section level2 unnumbered">
&lt;h2>References&lt;/h2>
&lt;div id="refs" class="references csl-bib-body hanging-indent">
&lt;div id="ref-mvt" class="csl-entry">
Genz, Alan, Frank Bretz, Tetsuhisa Miwa, Xuefei Mi, Friedrich Leisch, Fabian Scheipl, and Torsten Hothorn. 2019. &lt;em>&lt;span class="nocase">mvtnorm&lt;/span>: Multivariate Normal and t Distributions&lt;/em>. &lt;a href="https://CRAN.R-project.org/package=mvtnorm">https://CRAN.R-project.org/package=mvtnorm&lt;/a>.
&lt;/div>
&lt;div id="ref-heiss2008likelihood" class="csl-entry">
Heiss, Florian, and Viktor Winschel. 2008. &lt;span>“Likelihood Approximation by Numerical Integration on Sparse Grids.”&lt;/span> &lt;em>Journal of Econometrics&lt;/em> 144 (1): 62–80.
&lt;/div>
&lt;div id="ref-skrainka2011high" class="csl-entry">
Skrainka, Benjamin S, and Kenneth L Judd. 2011. &lt;span>“High Performance Quadrature Rules: How Numerical Integration Affects a Popular Model of Product Differentiation.”&lt;/span> &lt;em>Available at SSRN 1870703&lt;/em>.
&lt;/div>
&lt;/div>
&lt;/div></description></item><item><title>The Homotopy Principle: Simple Examples</title><link>https://jmodeler.github.io/post/the-homotopy-principle-simple-examples/</link><pubDate>Mon, 20 Mar 2023 00:00:00 +0000</pubDate><guid>https://jmodeler.github.io/post/the-homotopy-principle-simple-examples/</guid><description>
&lt;p>This post presents 2 simple examples of the homotopy principle applied to (fairly easy) linear and nonlinear systems of equations. At a very abstract level, given a system of equations for which a solution is needed, we convert this system to one whose solution we already know (or is easy to find out), and then bend this system till we get the solution to the original set of equations. These methods have very broad applications &lt;span class="citation">(&lt;a href="#ref-garciapathways" role="doc-biblioref">Garcia and Zangwill 1981&lt;/a>)&lt;/span>, and have been applied in the context of finding equlibria in Static Games &lt;span class="citation">(&lt;a href="#ref-Bajari2010" role="doc-biblioref">Bajari et al. 2010&lt;/a>)&lt;/span> and Dynamic Games &lt;span class="citation">(&lt;a href="#ref-Borkovsky2010" role="doc-biblioref">Borkovsky, Doraszelski, and Kryukov 2010&lt;/a>)&lt;/span>.&lt;/p>
&lt;div id="example-1-linear-system" class="section level2">
&lt;h2>Example 1: Linear System&lt;/h2>
&lt;p>Say we want to find the solution to the following system of linear equations:
&lt;span class="math display" id="eq:linEx1">\[\begin{align}
\left[ \begin{array}
{rrrrr}
1 &amp;amp; 2 \\
3 &amp;amp; 4 \\
\end{array}\right] \left[ \begin{array} {r}
x_1 \\
x_2 \\
\end{array} \right] = \left[ \begin{array} {l}
5 \\
11 \\
\end{array} \right] \tag{1}
\end{align}\]&lt;/span>&lt;/p>
&lt;p>Readers should very easily be able to verify that the unique solution to this system is
&lt;span class="math display">\[
(x_1,x_2) = (1,2)
\]&lt;/span>&lt;/p>
&lt;p>Let’s convert this system and introduce an additional parameter &lt;span class="math inline">\(t\)&lt;/span>, called the &lt;em>homotopy parameter&lt;/em>, which varies from &lt;span class="math inline">\(0\)&lt;/span> to &lt;span class="math inline">\(1\)&lt;/span>. Let’s call this new system &lt;span class="math inline">\(H(x_1, x_2, t)\)&lt;/span>
&lt;span class="math display" id="eq:HFunc">\[\begin{align}
\left[ \begin{array}
{rr}
1 &amp;amp; 2 \\
3 &amp;amp; 4 \\
\end{array}\right] \left[ \begin{array} {r}
x_1 \\
x_2 \\
\end{array} \right] = \left[ \begin{array} {l}
5t \\
11t \\
\end{array} \right] \tag{2}
\end{align}\]&lt;/span>&lt;/p>
&lt;p>When &lt;span class="math inline">\(t=0\)&lt;/span>, &lt;span class="math inline">\(H(x_1, x_2, 0)\)&lt;/span> yields the trivial (and only) solution &lt;span class="math inline">\((x_1,x_2) = (0,0)\)&lt;/span>. When &lt;span class="math inline">\(t=1\)&lt;/span>, we get our original system of equations back. When we solve for &lt;span class="math inline">\((x_1, x_2)\)&lt;/span> as a function of &lt;span class="math inline">\(t\)&lt;/span>, we get:
&lt;span class="math display">\[
(x_1(t),x_2(t)) = (t,2t)
\]&lt;/span>
At &lt;span class="math inline">\(t=1\)&lt;/span>, this will give us the solution we desire. Tracing the path of the solution gives us the following plots:
&lt;img src="https://jmodeler.github.io/post/the-homotopy-principle-simple-examples/index.en_files/figure-html/unnamed-chunk-1-1.png" width="672" />&lt;/p>
&lt;p>&lt;img src="https://jmodeler.github.io/post/the-homotopy-principle-simple-examples/index.en_files/figure-html/unnamed-chunk-2-1.png" width="672" />&lt;/p>
&lt;p>&lt;img src="https://jmodeler.github.io/post/the-homotopy-principle-simple-examples/index.en_files/figure-html/unnamed-chunk-3-1.png" width="672" />&lt;/p>
&lt;p>From this very simple example, we note that the general process followed is given below &lt;span class="citation">(&lt;a href="#ref-garciapathways" role="doc-biblioref">Garcia and Zangwill 1981&lt;/a>)&lt;/span>:&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>Convert the system of equations into one that has a known solution (i.e. &lt;span class="math inline">\(H(x_1, x_2, 0)\)&lt;/span> case above)&lt;/li>
&lt;li>Introduce a new parameter &lt;span class="math inline">\(t\)&lt;/span>, that gives the known system at &lt;span class="math inline">\(t=0\)&lt;/span> and the system for which the solutions are desired when &lt;span class="math inline">\(t=1\)&lt;/span>&lt;/li>
&lt;li>Trace the path of the solutions by changing the value of &lt;span class="math inline">\(t\)&lt;/span> from &lt;span class="math inline">\(0\)&lt;/span> to &lt;span class="math inline">\(1\)&lt;/span>&lt;/li>
&lt;/ol>
&lt;/div>
&lt;div id="example-2-nonlinear-system" class="section level2">
&lt;h2>Example 2: Nonlinear System&lt;/h2>
&lt;p>Consider the following system of equations (from chapter 1, exercise 6 of &lt;span class="citation">(&lt;a href="#ref-garciapathways" role="doc-biblioref">Garcia and Zangwill 1981&lt;/a>)&lt;/span>):
&lt;span class="math display" id="eq:nonlinEx1">\[\begin{align}
F(x_1, x_2) =
\left[ \begin{array}
{l}
e^{2 x_1} - x^{2}_{2} + 3 \\
4x_{2}e^{2 x_1} - x^{3}_{2} \\
\end{array} \right]
= \left[ \begin{array} {l}
0 \\
0 \\
\end{array} \right] \tag{3} \\
(x_1, x_2) \in \mathbb{R}^2 \\
F: \mathbb{R}^2 \rightarrow \mathbb{R}^2
\end{align}\]&lt;/span>&lt;/p>
&lt;p>Again, readers should verify that the solutions to this system of equations are &lt;span class="math inline">\((x1, x2) = (0, -2) \, \&amp;amp; \, (x1, x2) = (0, 2)\)&lt;/span>.&lt;/p>
&lt;p>We now introduce the homotopy parameter &lt;span class="math inline">\(t\)&lt;/span>, and define &lt;span class="math inline">\(H(x_1, x_2, t)\)&lt;/span> as:&lt;/p>
&lt;p>&lt;span class="math display" id="eq:HfuncNonlin1">\[\begin{align}
H(x_1, x_2, t) = F(x_1, x_2) - (1-t)F(0, 0) \tag{4}
\end{align}\]&lt;/span>&lt;/p>
&lt;p>The formulation in &lt;a href="#eq:HfuncNonlin1">(4)&lt;/a> is called the &lt;em>Newton Homotopy&lt;/em> &lt;span class="citation">(&lt;a href="#ref-garciapathways" role="doc-biblioref">Garcia and Zangwill 1981&lt;/a>)&lt;/span>. A distinct advantage of this formulation, is that at &lt;span class="math inline">\(t=0\)&lt;/span>, it is easy to see that the solution to the system is &lt;span class="math inline">\((0, 0)\)&lt;/span>.&lt;/p>
&lt;p>For any &lt;span class="math inline">\(t\)&lt;/span> between &lt;span class="math inline">\(0\)&lt;/span> and &lt;span class="math inline">\(1\)&lt;/span>, &lt;span class="math inline">\(H(x_1, x_2, t)\)&lt;/span> becomes:
&lt;span class="math display" id="eq:HfuncNonlin2">\[\begin{align}
\left[ \begin{array}
{l}
e^{2 x_1} - x^{2}_{2} + 4t - 1 \\
4x_{2}e^{2 x_1} - x^{3}_{2} \\
\end{array} \right]
= \left[ \begin{array} {l}
0 \\
0 \\
\end{array} \right]
\tag{5}
\end{align}\]&lt;/span>&lt;/p>
&lt;p>We now attempt to find &lt;span class="math inline">\((x_1, x_2)\)&lt;/span> as functions of &lt;span class="math inline">\(t\)&lt;/span>. From &lt;a href="#eq:HfuncNonlin2">(5)&lt;/a>, we have:
&lt;span class="math display" id="eq:x2Sol">\[\begin{align}
x_{2}^{3} = 4x_{2}e^{2 x_1} \nonumber \\
x_2 = 0 \,\,\,\,\, OR \,\,\,\,\, x_{2} = \pm 2e^{x_1} \tag{6}
\end{align}\]&lt;/span>&lt;/p>
&lt;p>When &lt;span class="math inline">\(x_2 = 0\)&lt;/span>, putting this back in &lt;a href="#eq:HfuncNonlin2">(5)&lt;/a> we get:
&lt;span class="math display" id="eq:x1Sol1">\[\begin{align}
e^{2 x_1} + 4t - 1 = 0 \nonumber \\
\implies x_1 = \frac{1}{2} log(1-4t) \tag{7} \\
where \,\,\,\, 0 \le t \le 1/4
\end{align}\]&lt;/span>&lt;/p>
&lt;p>When &lt;span class="math inline">\(x_2 = \pm 2e^{x_1}\)&lt;/span>, putting this back in &lt;a href="#eq:HfuncNonlin2">(5)&lt;/a> we get:
&lt;span class="math display" id="eq:x1Sol2">\[\begin{align}
-3e^{2 x_1} + 4t - 1 = 0 \nonumber \\
\implies x_1 = \frac{1}{2} log\left(\frac{4t-1}{3}\right) \tag{8} \\
where \,\,\,\, 1/4 &amp;lt; t \le 1
\end{align}\]&lt;/span>&lt;/p>
&lt;p>Combining all the findings from &lt;a href="#eq:x2Sol">(6)&lt;/a>, &lt;a href="#eq:x1Sol1">(7)&lt;/a> and &lt;a href="#eq:x1Sol2">(8)&lt;/a>, we get:
&lt;span class="math display" id="eq:x2t" id="eq:x1t">\[\begin{align}
x_1(t) = \begin{cases}
\frac{1}{2} log(1-4t) &amp;amp; \text{for } 0\le t \le 1/4\\
\frac{1}{2} log\left(\frac{4t-1}{3}\right) &amp;amp; \text{for } 1/4 &amp;lt; t \leq 1
\end{cases} \tag{9} \\
x_1(t) = \begin{cases}
0 &amp;amp; \text{for } 0\le t \le 1/4\\
\pm 2 \sqrt{\left(\frac{4t-1}{3}\right)} &amp;amp; \text{for } 1/4 &amp;lt; t \leq 1
\end{cases} \tag{10}
\end{align}\]&lt;/span>&lt;/p>
&lt;p>Which gives us the solution to the system of equations in &lt;a href="#eq:nonlinEx1">(3)&lt;/a> at &lt;span class="math inline">\(t = 1\)&lt;/span>. However, note that the functions &lt;span class="math inline">\(x_1(t), x_2(t)\)&lt;/span> are non-differentiable, which disqualifies them from being solution paths &lt;span class="citation">(&lt;a href="#ref-garciapathways" role="doc-biblioref">Garcia and Zangwill 1981&lt;/a>)&lt;/span>. This is evident in the plots shown below:
&lt;img src="https://jmodeler.github.io/post/the-homotopy-principle-simple-examples/index.en_files/figure-html/unnamed-chunk-4-1.png" width="672" />&lt;/p>
&lt;p>&lt;img src="https://jmodeler.github.io/post/the-homotopy-principle-simple-examples/index.en_files/figure-html/unnamed-chunk-5-1.png" width="672" />&lt;/p>
&lt;p>One could always try another formulation for &lt;span class="math inline">\(H(x_1, x_2, t)\)&lt;/span> which leads to well defined paths to the desired solution from the known solution (i.e. the solution to &lt;span class="math inline">\(H(x_1, x_2, 0)\)&lt;/span>). That is left as an exercise to the reader.&lt;/p>
&lt;/div>
&lt;div id="references" class="section level2 unnumbered">
&lt;h2>References&lt;/h2>
&lt;div id="refs" class="references csl-bib-body hanging-indent">
&lt;div id="ref-Bajari2010" class="csl-entry">
Bajari, Patrick, Han Hong, John Krainer, and Denis Nekipelov. 2010. &lt;span>“&lt;span class="nocase">Computing Equilibria in Static Games of Incomplete Information Using the All-Solution Homotopy&lt;/span>.”&lt;/span> &lt;em>Operations Research&lt;/em> 58 (4-part 2).
&lt;/div>
&lt;div id="ref-Borkovsky2010" class="csl-entry">
Borkovsky, Ron N., Ulrich Doraszelski, and Yaroslav Kryukov. 2010. &lt;span>“&lt;span class="nocase">A user’s guide to solving dynamic stochastic games using the homotopy method&lt;/span>.”&lt;/span> &lt;em>Operations Research&lt;/em> 58 (4 PART 2): 1116–32. &lt;a href="https://doi.org/10.1287/opre.1100.0843">https://doi.org/10.1287/opre.1100.0843&lt;/a>.
&lt;/div>
&lt;div id="ref-garciapathways" class="csl-entry">
Garcia, C B, and W I Zangwill. 1981. &lt;span>“&lt;span class="nocase">Pathways to solutions, fixed points, and equilibria. 1981&lt;/span>.”&lt;/span> Prentice-Hall, Englewood Cliffs, NJ.
&lt;/div>
&lt;/div>
&lt;/div></description></item><item><title>Completing Projects at Work</title><link>https://jmodeler.github.io/post/completing-projects-at-work/</link><pubDate>Sun, 19 Mar 2023 00:00:00 +0000</pubDate><guid>https://jmodeler.github.io/post/completing-projects-at-work/</guid><description>
&lt;p>This is a post inspired by a conversation I had with a colleague, Ryan. Over a very short period, he had completed an impressive portfolio of work projects, and always had something better in the pipeline. I asked him what he did when starting on anything, and he very kindly shared his method to get (most) things done at work (and in life, where applicable). In the following post, anything marked in &lt;span style="color: Tomato;"> tomato-red &lt;/span> is input from him.&lt;/p>
&lt;p>Ryan’s method to do anything in life (abstract version):&lt;/p>
&lt;ol style="list-style-type: decimal">
&lt;li>&lt;p>Determine what is being asked of you (or what you want to achieve). What problem are you trying to solve? Do not fall victim to the &lt;a href="https://en.wikipedia.org/wiki/XY_problem">XY problem&lt;/a>, be very clear about the deliverable expected. Check and re-check (or re-assess, in case this is a personal goal) with your boss/business partner/end goal &lt;span style="color: Tomato;">regularly&lt;/span>.&lt;/p>&lt;/li>
&lt;li>&lt;p>With a clear deliverable in mind, now clearly formulate the steps to be taken to get closer to said deliverable&lt;/p>&lt;/li>
&lt;li>&lt;p>Now, do you have all the skills/knowledge to do the steps yourself? Great! Make a plan to work on each step and stick to it. &lt;span style="color: Tomato;">Try to make a plan on how much work will be required time wise to progress through each step. This will also help you learn how good (or bad) you are at estimating time/specific tasks.&lt;/span>
&lt;span style="color: Tomato;"> I highly recommend reading &lt;a href="https://archive.fo/kHhdM">this blog post&lt;/a> about what to tackle first. He recommends basically making the smallest version of what you need that includes all the component parts. This helps you discover compatibility/integration issues up front that can wreck a project if they’re discovered late.&lt;br />
&lt;/span>&lt;/p>&lt;/li>
&lt;li>&lt;p>If you see there are gaps in your knowledge, you have two options:&lt;/p>
&lt;ul>
&lt;li>Learn everything you need to know to plug the knowledge gap (&lt;span style="color: Tomato;">generally terrifying and not recommended&lt;/span>)&lt;/li>
&lt;li>Find someone else who had the same problem and try to replicate what they’ve done. Google is your friend! (&lt;span style="color: Tomato;">generally where I start, using a &lt;a href="https://archive.fo/x5C54">tracer bullet&lt;/a>&lt;/span>). Once you’ve found a solution, follow the process outlined in step 3. and make progress on your work.&lt;/li>
&lt;/ul>&lt;/li>
&lt;li>&lt;p>As you keep working on your steps, DOCUMENT EVERYTHING. This will help you do the following:&lt;/p>
&lt;ul>
&lt;li>If you need to (or want to) move on to another project, you can hand this over to someone else and easily create a succession plan.&lt;/li>
&lt;li>If this project is a personal goal, you have a record of what you did. This will serve as a reference when you need to redo something like this in the future.&lt;/li>
&lt;li>&lt;span style="color: Tomato;">If your memory is anything like mine, it’s not great and you work on a lot. Your own documentation will help you know why you did something&lt;/span>.&lt;/li>
&lt;li>&lt;span style="color: Tomato;">It helps guide others through how to learn a topic&lt;/span>.&lt;/li>
&lt;li>&lt;span style="color: Tomato;">It can serve as a record of what work was done which can be useful as an archive, or for ideas for other work. Was one process a nightmare that you spent half your time on? Maybe that should be tackled next&lt;/span>.&lt;/li>
&lt;/ul>&lt;/li>
&lt;/ol></description></item></channel></rss>